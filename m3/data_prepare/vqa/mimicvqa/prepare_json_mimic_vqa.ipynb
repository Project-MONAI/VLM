{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "    \n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(path, data):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "def save_jsonl(path, data):\n",
    "    with open(path, 'w') as f:\n",
    "        for line in data:\n",
    "            json.dump(line, f)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process created json files for inference and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the vqa_json_creator.py script, we have json files containing multiple QA pairs for each image and consistent with the LLaVA-Med format. In this notebook, we will process these json files to create finalized json file for inference and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add relative file path to the image ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_relative_path(input_file, output_file, path_table_file):\n",
    "    data = load_json(input_file)\n",
    "    path_table = load_json(path_table_file)\n",
    "    new_data = [{'image': path_table[i['image']], \n",
    "                 'id': i['id'], \n",
    "                 'conversations': list(i['conversations'])} for i in data]\n",
    "    save_json(output_file, new_data)\n",
    "\n",
    "# usage\n",
    "# input and output json files\n",
    "input_json = Path(\"input_json\") / \"llava_med_instruct_mimicvqa_test_expertmodel.json\"\n",
    "output_json = Path(\"output_json\") / \"llava_med_instruct_mimicvqa_test_expertmodel_path.json\"\n",
    "# json file mapping each image ID to its relative path\n",
    "path_table_json = \"mimic_cxr_relpath.json\"\n",
    "add_relative_path(input_json, output_json, path_table_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split QA pairs into separate json files based on question type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract QA pairs from an old QA list which are of a specific question type at image level\n",
    "def select_conversation(old_list, qtype, qtable):\n",
    "    new_list = []\n",
    "    prefix = ''\n",
    "    suffix = f'\\n<image>'\n",
    "    for i in range(0, int(len(old_list) / 2)):\n",
    "        if i == 0:\n",
    "            text = old_list[i * 2]['value']\n",
    "            test_split = text.split('\\n')\n",
    "            if len(test_split) == 2:\n",
    "                prefix = ''\n",
    "            elif len(test_split) == 3:\n",
    "                prefix = f\"{test_split[0]}\\n\"\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid prefix: {text}\")\n",
    "\n",
    "        question = (old_list[i * 2]['value'].split('?')[0]).split('\\n')[-1]\n",
    "        if qtable[f'{question}?'] == qtype:\n",
    "            # add expertise and image\n",
    "            if len(new_list) == 0:\n",
    "                new_item = old_list[i * 2]\n",
    "                new_item['value'] = f\"{prefix}{question}?{suffix}\"\n",
    "                new_list.append(new_item)\n",
    "                new_list.append(old_list[i * 2 + 1])                \n",
    "            else:\n",
    "                new_list.append(old_list[i * 2])\n",
    "                new_list.append(old_list[i * 2 + 1])\n",
    "    return new_list\n",
    "\n",
    "# extract QA pairs from a list of QA pairs which are of a specific question type for the whole json\n",
    "def select_qa(input_file, output_file, question_table_file, question_type):\n",
    "    data = load_json(input_file)\n",
    "    qtable = load_json(question_table_file)\n",
    "    new_data = []\n",
    "    for i in data:\n",
    "        new_conversations = select_conversation(i['conversations'], question_type, qtable)\n",
    "        if len(new_conversations) > 0:\n",
    "            new_data.append({'image': i['image'], \n",
    "                             'id': i['id'], \n",
    "                             'conversations': new_conversations})\n",
    "    \n",
    "    save_json(output_file, new_data)\n",
    "\n",
    "# usage\n",
    "question_types = ['abnormality', 'level', 'location', 'presence', 'type', 'view']\n",
    "question_table_file = 'mimic_vqa_qtype.json'\n",
    "root = Path(\"output_json\")\n",
    "\n",
    "for qtype in question_types:\n",
    "    output_dir = root / qtype\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir(parents=True)\n",
    "    \n",
    "    # with expert model\n",
    "    input_file = root / 'llava_med_instruct_mimicvqa_test_expertmodel_path.json'\n",
    "    output_file = output_dir / 'llava_med_instruct_mimicvqa_test_expert.json'\n",
    "    select_qa(input_file, output_file, question_table_file, qtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These JSON files can now be used for first-round inference. However, during training, expert information is only included in the first question of each QA conversation round. Therefore, to generate the inference correctly, the following changes need to be made:\n",
    "\n",
    "* If a question q is not the first question in a QA conversation, a second-round inference is required.\n",
    "* In this second-round inference, the first question in that QA conversation, along with its predicted answer from the first-round inference and the expert information, should be inserted before the question q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert first-round inference answers to a json\n",
    "def create_dict_id_predictions(input_file):\n",
    "    data = load_jsonl(input_file)\n",
    "    res = {}\n",
    "    for i in data:\n",
    "        key = f\"{i['question_id']}_{i['prompt']}\"\n",
    "        res[key] = i['text']\n",
    "    return res\n",
    "\n",
    "question_types = ['abnormality', 'presence', 'view', 'location', 'level', 'type']\n",
    "predictions_root = Path(\"../predicted_vqa_json/ckpt_llava_med_all_mimic_expert_1run\")\n",
    "predictions = {}\n",
    "for qtype in question_types:\n",
    "    input_file = predictions_root / qtype / 'llava_med_expert_all_mimic_expert_run1.jsonl'\n",
    "    predictions.update(create_dict_id_predictions(input_file))\n",
    "\n",
    "save_json('predictions.json', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert first-round inference (predictions) and expert information into each QA pair\n",
    "def add_expert_prompt(image_id, old_list, qtype, qtable, predictions):\n",
    "    new_list = []\n",
    "    prefix = ''\n",
    "    suffix = ''\n",
    "    for i in range(0, int(len(old_list) / 2)):\n",
    "        if i == 0:\n",
    "            text = old_list[i * 2]['value']\n",
    "            text_split = text.split('\\n')\n",
    "            # pred_key = f\"{text.split('<image>')[0]}<patch_token_holder>\\n<image>\"\n",
    "            # pred_answer = predictions[f\"{image_id}_{pred_key}\"]\n",
    "            pred_answer = predictions[f\"{image_id}_{text}\"]\n",
    "            if len(text_split) == 2:\n",
    "                expert = ''\n",
    "                question = text_split[0]\n",
    "            elif len(text_split) == 3:\n",
    "                expert = f\"{text_split[0]}\\n\"\n",
    "                question = text_split[1]\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid first question prompt: {text}\")\n",
    "            prefix = f\"{expert}{question}\\n<patch_token_holder>\\n###{pred_answer}\\n###Human: \"\n",
    "            # check question type \n",
    "            if qtable[f'{question}'] == qtype:\n",
    "                new_item = old_list[i * 2]\n",
    "                new_item['value'] = f\"{expert}{question}\\n<patch_token_holder>\"\n",
    "                new_list.append(new_item)\n",
    "                new_list.append(old_list[i * 2 + 1])\n",
    "        else:\n",
    "            question = old_list[i * 2]['value']\n",
    "            if qtable[f'{question}'] == qtype:\n",
    "                # add expertise and image\n",
    "                if len(new_list) == 0:\n",
    "                    new_item = old_list[i * 2]\n",
    "                    new_item['value'] = f\"{prefix}{question}{suffix}\"\n",
    "                    new_list.append(new_item)\n",
    "                    new_list.append(old_list[i * 2 + 1])                \n",
    "                else:\n",
    "                    new_list.append(old_list[i * 2])\n",
    "                    new_list.append(old_list[i * 2 + 1])\n",
    "    return new_list\n",
    "\n",
    "# apply to the whole json file\n",
    "def add_expert_prompt_json(input_file, output_file, question_table_file, question_type, predictions_file):\n",
    "    data = load_json(input_file)\n",
    "    qtable = load_json(question_table_file)\n",
    "    predictions = load_json(predictions_file)\n",
    "    new_data = []\n",
    "    for i in data:\n",
    "        new_conversations = add_expert_prompt(i['id'], i['conversations'], question_type, qtable, predictions)\n",
    "        if len(new_conversations) > 0:\n",
    "            new_data.append({'image': i['image'], \n",
    "                             'id': i['id'], \n",
    "                             'conversations': new_conversations})\n",
    "    \n",
    "    save_json(output_file, new_data)\n",
    "\n",
    "# usage\n",
    "question_types = ['abnormality', 'presence', 'view', 'location', 'level', 'type']\n",
    "question_table_file = 'mimic_vqa_qtype.json'\n",
    "predictions_file = 'predictions.json'\n",
    "input_file = Path('output_json') / 'llava_med_instruct_mimicvqa_test_expertmodel_path.json'\n",
    "for qtype in question_types:\n",
    "    output_dir = Path('output_json') / qtype\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir(parents=True)\n",
    "    \n",
    "    # with expert model\n",
    "    output_file = output_dir / 'llava_med_instruct_mimicvqa_test_expert_2run.json'\n",
    "    add_expert_prompt_json(input_file, output_file, question_table_file, qtype, predictions_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add question and answer type to testing json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_qa_type(input_file, output_file, question_type):\n",
    "    data = load_json(input_file)\n",
    "    new_data = []\n",
    "    for i in data:\n",
    "        question = (i['conversations'][0]['value'].split('?')[0]).split('\\n')[-1]\n",
    "        answer = i['conversations'][1]['value'].lower()\n",
    "        if answer == 'yes' or answer == 'no':\n",
    "            closed = 'closed'\n",
    "        elif 'yes' in answer or 'no' in answer:\n",
    "            closed = 'closed'\n",
    "            print(f\"Warning ambiguous question: {question} {answer}\")\n",
    "        else:\n",
    "            closed = 'open'\n",
    "\n",
    "        new_data.append({\n",
    "            'image': i['image'], \n",
    "            'id': i['id'], \n",
    "            'conversations': i['conversations'],\n",
    "            'question_type': question_type,\n",
    "            'answer_type': closed \n",
    "        })\n",
    "\n",
    "    save_json(output_file, new_data)\n",
    "\n",
    "# usage\n",
    "question_types = ['abnormality', 'level', 'location', 'presence', 'type', 'view']\n",
    "root = Path('output_json')\n",
    "\n",
    "for qtype in question_types:\n",
    "    output_dir = root / qtype\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir(parents=True)\n",
    "    \n",
    "    # with expert model\n",
    "    input_file = output_dir / 'llava_med_instruct_mimicvqa_test_expert.json'\n",
    "    output_file = output_dir / 'llava_med_instruct_mimicvqa_test_expert_type.json'\n",
    "    add_qa_type(input_file, output_file, qtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zj-gitdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
